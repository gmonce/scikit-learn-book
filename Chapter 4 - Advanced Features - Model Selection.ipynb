{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Learning Scikit-learn: Machine Learning in Python"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Notebook for Chapter 4: Advanced Features - Model Selection"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%pylab inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Populating the interactive namespace from numpy and matplotlib\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.datasets import fetch_20newsgroups\n",
      "\n",
      "news = fetch_20newsgroups(subset='all')\n",
      "\n",
      "n_samples = 3000\n",
      "\n",
      "X = news.data[:n_samples]\n",
      "y = news.target[:n_samples]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.naive_bayes import MultinomialNB\n",
      "from sklearn.pipeline import Pipeline\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_stop_words():\n",
      "    result = set()\n",
      "    for line in open('stopwords_en.txt', 'r').readlines():\n",
      "        result.add(line.strip())\n",
      "    return result\n",
      "\n",
      "stop_words = get_stop_words()\n",
      "\n",
      "clf = Pipeline([\n",
      "    ('vect', TfidfVectorizer(\n",
      "                stop_words=stop_words,\n",
      "                token_pattern=ur\"\\b[a-z0-9_\\-\\.]+[a-z][a-z0-9_\\-\\.]+\\b\",         \n",
      "    )),\n",
      "    ('nb', MultinomialNB(alpha=0.01)),\n",
      "])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "IOError",
       "evalue": "[Errno 2] No such file or directory: 'stopwords_en.txt'",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-4-691b1beebbba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mstop_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_stop_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m clf = Pipeline([\n",
        "\u001b[0;32m<ipython-input-4-691b1beebbba>\u001b[0m in \u001b[0;36mget_stop_words\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_stop_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'stopwords_en.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'stopwords_en.txt'"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.cross_validation import cross_val_score, KFold\n",
      "from scipy.stats import sem\n",
      "\n",
      "def evaluate_cross_validation(clf, X, y, K):\n",
      "    # create a k-fold croos validation iterator of k=5 folds\n",
      "    cv = KFold(len(y), K, shuffle=True, random_state=0)\n",
      "    # by default the score used is the one returned by score method of the estimator (accuracy)\n",
      "    scores = cross_val_score(clf, X, y, cv=cv)\n",
      "    print scores\n",
      "    print (\"Mean score: {0:.3f} (+/-{1:.3f})\").format(\n",
      "        np.mean(scores), sem(scores))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "evaluate_cross_validation(clf, X, y, 3)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def calc_params(X, y, clf, param_values, param_name, K):\n",
      "    # initialize training and testing scores with zeros\n",
      "    train_scores = np.zeros(len(param_values))\n",
      "    test_scores = np.zeros(len(param_values))\n",
      "    \n",
      "    # iterate over the different parameter values\n",
      "    for i, param_value in enumerate(param_values):\n",
      "        print param_name, ' = ', param_value\n",
      "        \n",
      "        # set classifier parameters\n",
      "        clf.set_params(**{param_name:param_value})\n",
      "        \n",
      "        # initialize the K scores obtained for each fold\n",
      "        k_train_scores = np.zeros(K)\n",
      "        k_test_scores = np.zeros(K)\n",
      "        \n",
      "        # create KFold cross validation\n",
      "        cv = KFold(n_samples, K, shuffle=True, random_state=0)\n",
      "        \n",
      "        # iterate over the K folds\n",
      "        for j, (train, test) in enumerate(cv):\n",
      "            # fit the classifier in the corresponding fold\n",
      "            # and obtain the corresponding accuracy scores on train and test sets\n",
      "            clf.fit([X[k] for k in train], y[train])\n",
      "            k_train_scores[j] = clf.score([X[k] for k in train], y[train])\n",
      "            k_test_scores[j] = clf.score([X[k] for k in test], y[test])\n",
      "            \n",
      "        # store the mean of the K fold scores\n",
      "        train_scores[i] = np.mean(k_train_scores)\n",
      "        test_scores[i] = np.mean(k_test_scores)\n",
      "       \n",
      "    # plot the training and testing scores in a log scale\n",
      "    plt.semilogx(param_values, train_scores, alpha=0.4, lw=2, c='b')\n",
      "    plt.semilogx(param_values, test_scores, alpha=0.4, lw=2, c='g')\n",
      "    \n",
      "    plt.xlabel(param_name + \" values\")\n",
      "    plt.ylabel(\"Mean cross validation accuracy\")\n",
      "\n",
      "    # return the training and testing scores on each parameter value\n",
      "    return train_scores, test_scores"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "alphas = np.logspace(-7, 0, 8)\n",
      "print alphas"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_scores, test_scores = calc_params(X, y, clf, alphas, 'nb__alpha', 3)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'training scores: ', train_scores\n",
      "print 'testing scores: ', test_scores"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.svm import SVC\n",
      "\n",
      "clf = Pipeline([\n",
      "    ('vect', TfidfVectorizer(\n",
      "                stop_words=stop_words,\n",
      "                token_pattern=ur\"\\b[a-z0-9_\\-\\.]+[a-z][a-z0-9_\\-\\.]+\\b\",         \n",
      "    )),\n",
      "    ('svc', SVC()),\n",
      "])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "gammas = np.logspace(-2, 1, 4)\n",
      "\n",
      "train_scores, test_scores = calc_params(X, y, clf, gammas, 'svc__gamma', 3)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'training scores: ', train_scores\n",
      "print 'testing scores: ', test_scores"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For gamma < 1 we have underfitting. For gamma > 1 we have overfitting. So here, the best result is for gamma = 1 where we obtain a training an accuracy of 0.999 and a testing accuracy of 0.760"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.grid_search import GridSearchCV\n",
      "\n",
      "parameters = {\n",
      "    'svc__gamma': np.logspace(-2, 1, 4),\n",
      "    'svc__C': np.logspace(-1, 1, 3),\n",
      "}\n",
      "\n",
      "clf = Pipeline([\n",
      "    ('vect', TfidfVectorizer(\n",
      "                stop_words=stop_words,\n",
      "                token_pattern=ur\"\\b[a-z0-9_\\-\\.]+[a-z][a-z0-9_\\-\\.]+\\b\",         \n",
      "    )),\n",
      "    ('svc', SVC()),\n",
      "])\n",
      "\n",
      "gs = GridSearchCV(clf, parameters, verbose=2, refit=False, cv=3)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%time _ = gs.fit(X, y)\n",
      "\n",
      "gs.best_params_, gs.best_score_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "With the grid search we obtained a better combination of C and gamma parameters, for values 10.0 and 0.10 respectively, we obtained a 3-fold cross validation accuracy of 0.811 much better than the best value we obtained (0.76) in the previous experiment by only adjusting gamma and keeeping C value at 1.0."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We could continue trying to improve the results by also adjusting the vectorizer parameters in the grid search."
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Parallelizing"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "First we will declare a function that will persist all the K folds for the cross validation in different files. These files will be loaded by a process that will execute the corresponding fold:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.externals import joblib\n",
      "from sklearn.cross_validation import ShuffleSplit\n",
      "import os\n",
      "\n",
      "def persist_cv_splits(X, y, K=3, name='data', suffix=\"_cv_%03d.pkl\"):\n",
      "    \"\"\"Dump K folds to filesystem.\"\"\"\n",
      "    \n",
      "    cv_split_filenames = []\n",
      "    \n",
      "    # create KFold cross validation\n",
      "    cv = KFold(n_samples, K, shuffle=True, random_state=0)\n",
      "    \n",
      "    # iterate over the K folds\n",
      "    for i, (train, test) in enumerate(cv):\n",
      "        cv_fold = ([X[k] for k in train], y[train], [X[k] for k in test], y[test])\n",
      "        cv_split_filename = name + suffix % i\n",
      "        cv_split_filename = os.path.abspath(cv_split_filename)\n",
      "        joblib.dump(cv_fold, cv_split_filename)\n",
      "        cv_split_filenames.append(cv_split_filename)\n",
      "    \n",
      "    return cv_split_filenames"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cv_filenames = persist_cv_splits(X, y, name='news')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The following function loads a particular fold and fits the classifier with the specified parameters set. Finally returns the testing score. This function will be called bye each of the parallel processes:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def compute_evaluation(cv_split_filename, clf, params):\n",
      "    \n",
      "    # All module imports should be executed in the worker namespace\n",
      "    from sklearn.externals import joblib\n",
      "\n",
      "    # load the fold training and testing partitions from the filesystem\n",
      "    X_train, y_train, X_test, y_test = joblib.load(\n",
      "        cv_split_filename, mmap_mode='c')\n",
      "    \n",
      "    clf.set_params(**params)\n",
      "    clf.fit(X_train, y_train)\n",
      "    test_score = clf.score(X_test, y_test)\n",
      "    return test_score"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This function executes the grid search in parallel processes. For each of the parameter combination (returned by the IterGrid iterator), it iterates over the K folds and creates a process to compute the evaluation. It returns the parameter combinations alongside with the tasks list: "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.grid_search import IterGrid\n",
      "\n",
      "def parallel_grid_search(lb_view, clf, cv_split_filenames, param_grid):\n",
      "    \n",
      "    all_tasks = []\n",
      "    all_parameters = list(IterGrid(param_grid))\n",
      "    \n",
      "    # iterate over parameter combinations\n",
      "    for i, params in enumerate(all_parameters):\n",
      "        task_for_params = []\n",
      "        \n",
      "        # iterate over the K folds\n",
      "        for j, cv_split_filename in enumerate(cv_split_filenames):    \n",
      "            t = lb_view.apply(\n",
      "                compute_evaluation, cv_split_filename, clf, params)\n",
      "            task_for_params.append(t) \n",
      "        \n",
      "        all_tasks.append(task_for_params)\n",
      "        \n",
      "    return all_parameters, all_tasks"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "Now we use IPython parallel to get the client and a load balanced view. We must first create a local cluster of N engines by using the Cluster tab in the IPython notebook. Then we create the client, the view and execute our parallel_grid_search function:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.svm import SVC\n",
      "from IPython.parallel import Client\n",
      "\n",
      "client = Client()\n",
      "lb_view = client.load_balanced_view()\n",
      "\n",
      "all_parameters, all_tasks = parallel_grid_search(\n",
      "   lb_view, clf, cv_filenames, parameters)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def print_progress(tasks):\n",
      "    progress = np.mean([task.ready() for task_group in tasks\n",
      "                                 for task in task_group])\n",
      "    print \"Tasks completed: {0}%\".format(100 * progress)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print_progress(all_tasks)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def find_bests(all_parameters, all_tasks, n_top=5):\n",
      "    \"\"\"Compute the mean score of the completed tasks\"\"\"\n",
      "    mean_scores = []\n",
      "    \n",
      "    for param, task_group in zip(all_parameters, all_tasks):\n",
      "        scores = [t.get() for t in task_group if t.ready()]\n",
      "        if len(scores) == 0:\n",
      "            continue\n",
      "        mean_scores.append((np.mean(scores), param))\n",
      "                   \n",
      "    return sorted(mean_scores, reverse=True)[:n_top]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print find_bests(all_parameters, all_tasks)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}